import fire
import os
import glob
import json
import re
import pandas as pd
from geotext import GeoText
from configs import *

base_dir = BASE_DIR
feature_dir = os.path.join(base_dir, "user_features")

# ---------------------------------------------------------
# æ¬§æ´²å›½å®¶ï¼ˆåŒ…å«å¤šè¯­è¨€ aliasesï¼‰â†’ ISO ä»£ç æ˜ å°„
# ---------------------------------------------------------
eu_country_to_iso = {
    "albania": "AL",
    "andorra": "AD",
    "austria": "AT",
    "belarus": "BY",
    "belgium": "BE",
    "bosnia": "BA",
    "bulgaria": "BG",
    "croatia": "HR",
    "cyprus": "CY",
    "czechia": "CZ",
    "denmark": "DK",
    "estonia": "EE",
    "finland": "FI",
    "france": "FR",
    "germany": "DE",
    "greece": "GR",
    "hungary": "HU",
    "iceland": "IS",
    "ireland": "IE",
    "italy": "IT",
    "latvia": "LV",
    "liechtenstein": "LI",
    "lithuania": "LT",
    "luxembourg": "LU",
    "malta": "MT",
    "moldova": "MD",
    "monaco": "MC",
    "montenegro": "ME",
    "netherlands": "NL",
    "north macedonia": "MK",
    "norway": "NO",
    "poland": "PL",
    "portugal": "PT",
    "romania": "RO",
    "russia": "RU",
    "san marino": "SM",
    "serbia": "RS",
    "slovakia": "SK",
    "slovenia": "SI",
    "spain": "ES",
    "sweden": "SE",
    "switzerland": "CH",
    "ukraine": "UA",
    "united kingdom": "GB",
}

# å›½å®¶åˆ«åæ˜ å°„
eu_country_aliases = {
    "albania": ["albania", "shqipÃ«ria"],
    "andorra": ["andorra"],
    "austria": ["austria", "Ã¶sterreich"],
    "belarus": ["belarus", "byelorussia"],
    "belgium": ["belgium", "belgique", "belgiÃ«"],
    "bosnia": ["bosnia", "bosnia and herzegovina", "bih"],
    "bulgaria": ["bulgaria"],
    "croatia": ["croatia", "hrvatska"],
    "cyprus": ["cyprus"],
    "czechia": ["czechia", "czech republic", "Äesko", "cesko"],
    "denmark": ["denmark", "danmark"],
    "estonia": ["estonia", "eesti"],
    "finland": ["finland", "suomi"],
    "france": ["france", "francia"],
    "germany": ["germany", "deutschland"],
    "greece": ["greece", "hellas", "ÎµÎ»Î»Î¬Î´Î±"],
    "hungary": ["hungary", "magyarorszÃ¡g"],
    "iceland": ["iceland", "Ã­sland"],
    "ireland": ["ireland", "Ã©ire"],
    "italy": ["italy", "italia"],
    "latvia": ["latvia", "latvija"],
    "liechtenstein": ["liechtenstein"],
    "lithuania": ["lithuania", "lietuvĞ°"],
    "luxembourg": ["luxembourg", "letzebuerg"],
    "malta": ["malta"],
    "moldova": ["moldova"],
    "monaco": ["monaco"],
    "montenegro": ["montenegro"],
    "netherlands": ["netherlands", "holland", "nederland"],
    "north macedonia": ["north macedonia", "makedonija"],
    "norway": ["norway", "norge"],
    "poland": ["poland", "polska"],
    "portugal": ["portugal", "portuguesa"],
    "romania": ["romania", "romÃ¢nia"],
    "russia": ["russia", "Ñ€Ğ¾ÑÑĞ¸Ñ"],
    "san marino": ["san marino"],
    "serbia": ["serbia", "srbija"],
    "slovakia": ["slovakia", "slovensko"],
    "slovenia": ["slovenia", "slovenija"],
    "spain": ["spain", "espaÃ±a", "espana"],
    "sweden": ["sweden", "sverige"],
    "switzerland": ["switzerland", "schweiz", "suisse", "svizzera"],
    "ukraine": ["ukraine", "ÑƒĞºÑ€Ğ°Ñ—Ğ½Ğ°"],
    "united kingdom": ["united kingdom", "uk", "england", "scotland", "wales", "northern ireland", "gb", "britain"]
}

# åˆ›å»ºåˆ«ååˆ° ISO çš„æ˜ å°„
alias_to_iso = {}
for country, aliases in eu_country_aliases.items():
    iso = eu_country_to_iso[country]
    for alias in aliases:
        alias_to_iso[alias.lower()] = iso

# ---------------------------------------------------------
# å›½æ—— emoji â†’ ISO ä»£ç æ˜ å°„
# ---------------------------------------------------------
emoji_to_iso = {
    "ğŸ‡¦ğŸ‡±": "AL",  # Albania
    "ğŸ‡¦ğŸ‡©": "AD",  # Andorra
    "ğŸ‡¦ğŸ‡¹": "AT",  # Austria
    "ğŸ‡§ğŸ‡¾": "BY",  # Belarus
    "ğŸ‡§ğŸ‡ª": "BE",  # Belgium
    "ğŸ‡§ğŸ‡¦": "BA",  # Bosnia and Herzegovina
    "ğŸ‡§ğŸ‡¬": "BG",  # Bulgaria
    "ğŸ‡­ğŸ‡·": "HR",  # Croatia
    "ğŸ‡¨ğŸ‡¾": "CY",  # Cyprus
    "ğŸ‡¨ğŸ‡¿": "CZ",  # Czechia
    "ğŸ‡©ğŸ‡°": "DK",  # Denmark
    "ğŸ‡ªğŸ‡ª": "EE",  # Estonia
    "ğŸ‡«ğŸ‡®": "FI",  # Finland
    "ğŸ‡«ğŸ‡·": "FR",  # France
    "ğŸ‡©ğŸ‡ª": "DE",  # Germany
    "ğŸ‡¬ğŸ‡·": "GR",  # Greece
    "ğŸ‡­ğŸ‡º": "HU",  # Hungary
    "ğŸ‡®ğŸ‡¸": "IS",  # Iceland
    "ğŸ‡®ğŸ‡ª": "IE",  # Ireland
    "ğŸ‡®ğŸ‡¹": "IT",  # Italy
    "ğŸ‡±ğŸ‡»": "LV",  # Latvia
    "ğŸ‡±ğŸ‡®": "LI",  # Liechtenstein
    "ğŸ‡±ğŸ‡¹": "LT",  # Lithuania
    "ğŸ‡±ğŸ‡º": "LU",  # Luxembourg
    "ğŸ‡²ğŸ‡¹": "MT",  # Malta
    "ğŸ‡²ğŸ‡©": "MD",  # Moldova
    "ğŸ‡²ğŸ‡¨": "MC",  # Monaco
    "ğŸ‡²ğŸ‡ª": "ME",  # Montenegro
    "ğŸ‡³ğŸ‡±": "NL",  # Netherlands
    "ğŸ‡²ğŸ‡°": "MK",  # North Macedonia
    "ğŸ‡³ğŸ‡´": "NO",  # Norway
    "ğŸ‡µğŸ‡±": "PL",  # Poland
    "ğŸ‡µğŸ‡¹": "PT",  # Portugal
    "ğŸ‡·ğŸ‡´": "RO",  # Romania
    "ğŸ‡·ğŸ‡º": "RU",  # Russia
    "ğŸ‡¸ğŸ‡²": "SM",  # San Marino
    "ğŸ‡·ğŸ‡¸": "RS",  # Serbia
    "ğŸ‡¸ğŸ‡°": "SK",  # Slovakia
    "ğŸ‡¸ğŸ‡®": "SI",  # Slovenia
    "ğŸ‡ªğŸ‡¸": "ES",  # Spain
    "ğŸ‡¸ğŸ‡ª": "SE",  # Sweden
    "ğŸ‡¨ğŸ‡­": "CH",  # Switzerland
    "ğŸ‡ºğŸ‡¦": "UA",  # Ukraine
    "ğŸ‡¬ğŸ‡§": "GB",  # United Kingdom
    "ğŸ‡»ğŸ‡¦": "VA",  # Vatican City
}

# ISO ä»£ç åˆ°å›½å®¶åç§°çš„æ˜ å°„ï¼ˆç”¨äº GeoText ç»“æœè½¬æ¢ï¼‰
iso_to_country_name = {
    "AL": "Albania",
    "AD": "Andorra",
    "AT": "Austria",
    "BY": "Belarus",
    "BE": "Belgium",
    "BA": "Bosnia and Herzegovina",
    "BG": "Bulgaria",
    "HR": "Croatia",
    "CY": "Cyprus",
    "CZ": "Czech Republic",
    "DK": "Denmark",
    "EE": "Estonia",
    "FI": "Finland",
    "FR": "France",
    "DE": "Germany",
    "GR": "Greece",
    "HU": "Hungary",
    "IS": "Iceland",
    "IE": "Ireland",
    "IT": "Italy",
    "LV": "Latvia",
    "LI": "Liechtenstein",
    "LT": "Lithuania",
    "LU": "Luxembourg",
    "MT": "Malta",
    "MD": "Moldova",
    "MC": "Monaco",
    "ME": "Montenegro",
    "NL": "Netherlands",
    "MK": "North Macedonia",
    "NO": "Norway",
    "PL": "Poland",
    "PT": "Portugal",
    "RO": "Romania",
    "RU": "Russia",
    "SM": "San Marino",
    "RS": "Serbia",
    "SK": "Slovakia",
    "SI": "Slovenia",
    "ES": "Spain",
    "SE": "Sweden",
    "CH": "Switzerland",
    "UA": "Ukraine",
    "GB": "United Kingdom",
    "VA": "Vatican City",
}

# å¼ºæ¬§æ´²åŸå¸‚ â†’ å›½å®¶æ˜ å°„
city_to_iso = {
    "london": "GB",
    "paris": "FR",
    "berlin": "DE",
    "rome": "IT",
    "madrid": "ES",
    "vienna": "AT",
    "amsterdam": "NL",
    "brussels": "BE",
    "stockholm": "SE",
    "copenhagen": "DK",
    "dublin": "IE",
    "oslo": "NO",
    "helsinki": "FI",
    "zurich": "CH",
    "geneva": "CH",
    "prague": "CZ",
    "budapest": "HU",
    "lisbon": "PT",
    "athens": "GR",
    "milan": "IT",
    "barcelona": "ES",
    "munich": "DE",
    "hamburg": "DE",
    "frankfurt": "DE",
    "krakow": "PL",
    "vilnius": "LT",
    "riga": "LV",
    "tallinn": "EE",
    "valencia": "ES",
    "manchester": "GB",
    "cambridge": "GB",
    "oxford": "GB",
}

prons = ["her", "she", "he", "him", "his", "they", "them", "bi", "hole", "black", "white", "gender", "fluid"]

strange = [
    "hell", "heaven", "twitter", "tiktok", "instagram", "facebook", "fuck", "planet",
    "alien", "aliens", "earth", "emotion", "mastodon", "ig", "tweet", "idk", "stardew",
    ".com", "podcast", "mcdonalds", "kfc", "universe"
]


def normalize(loc):
    """æ ‡å‡†åŒ–ä½ç½®å­—ç¬¦ä¸²"""
    loc = loc.lower().strip()
    loc = loc.replace(",", " ")
    loc = loc.replace(".", " ")
    loc = loc.replace("/", " ")
    loc = loc.replace("-", " ")
    loc = loc.replace("_", " ")
    loc = loc.replace("|", " ")
    loc = re.sub(r"\s+", " ", loc)
    # æ‰€æœ‰æ•°å­—éƒ½å»æ‰
    loc = re.sub(r"\d+", "", loc)
    # pronséƒ½å»æ‰
    for word in prons:
        if word.lower() in loc:
            loc = loc.replace(word.lower(), "")
    return loc


def identify_country(location):
    """
    è¯†åˆ«ä½ç½®æ‰€å±çš„å›½å®¶ï¼Œè¿”å› ISO ä»£ç 
    å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› None
    """
    loc_norm = normalize(location)
    if len(loc_norm) == 0:
        return None
    
    # 1. æ£€æŸ¥ emoji
    for emoji, iso in emoji_to_iso.items():
        if emoji in location:
            return iso
    
    # 2. æ£€æŸ¥å›½å®¶åˆ«å
    for alias, iso in alias_to_iso.items():
        if alias in loc_norm:
            return iso
    
    # 3. æ£€æŸ¥åŸå¸‚
    splitted = loc_norm.strip().split(" ")
    for word in splitted:
        if word in city_to_iso:
            return city_to_iso[word]
    
    # 4. æ£€æŸ¥ ISO ä»£ç ï¼ˆç›´æ¥å‡ºç°åœ¨ä½ç½®ä¸­ï¼‰
    for word in splitted:
        word_upper = word.upper()
        if word_upper in eu_country_to_iso.values():
            return word_upper
    
    # 5. ä½¿ç”¨ GeoText
    try:
        geotext_result = GeoText(location)
        country_mentions = geotext_result.country_mentions
        if country_mentions:
            # GeoText è¿”å›çš„æ˜¯å›½å®¶åç§°ï¼Œéœ€è¦è½¬æ¢ä¸º ISO
            for country_name in country_mentions:
                country_name_lower = country_name.lower()
                # å…ˆæ£€æŸ¥åˆ«åæ˜ å°„
                if country_name_lower in alias_to_iso:
                    return alias_to_iso[country_name_lower]
                # å†æ£€æŸ¥æ ‡å‡†å›½å®¶åç§°
                for country_key, iso in eu_country_to_iso.items():
                    if country_key in country_name_lower or country_name_lower in country_key:
                        return iso
    except:
        pass
    
    return None


def analyze_eu_locations_by_country():
    """
    åˆ†ææ¬§æ´²ä½ç½®ï¼ŒæŒ‰å›½å®¶åˆ†ç±»
    è¿”å›å­—å…¸ï¼š{ISOä»£ç : [ä½ç½®åˆ—è¡¨], "unknown": [æ— æ³•ç¡®å®šçš„ä½ç½®]}
    """
    # è¯»å–æ¬§æ´²ä½ç½®åˆ—è¡¨
    with open(os.path.join(base_dir, "eu_location_classified.json"), "r") as f:
        eu_location_data = json.load(f)
    
    eu_locations = eu_location_data.get("eu", [])
    
    # æŒ‰å›½å®¶åˆ†ç±»
    country_locations = {}
    unknown_locations = []
    
    print(f"å¼€å§‹åˆ†æ {len(eu_locations)} ä¸ªæ¬§æ´²ä½ç½®...")
    
    for loc in eu_locations:
        if not loc or not loc.strip():
            continue
        
        iso = identify_country(loc)
        
        if iso:
            if iso not in country_locations:
                country_locations[iso] = []
            country_locations[iso].append(loc)
        else:
            unknown_locations.append(loc)
    
    # æ·»åŠ  unknown é”®
    if unknown_locations:
        country_locations["unknown"] = unknown_locations
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n=== åˆ†æç»“æœç»Ÿè®¡ ===")
    for iso, locations in sorted(country_locations.items(), key=lambda x: len(x[1]), reverse=True):
        country_name = iso_to_country_name.get(iso, iso)
        print(f"{iso} ({country_name}): {len(locations)} ä¸ªä½ç½®")
    
    # ä¿å­˜ç»“æœ
    output_file = os.path.join(base_dir, "eu_location_by_country.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(country_locations, f, indent=2, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print(f"unknown: {unknown_locations}")
    
    # return country_locations


def merge_country_results():
    """
    åˆå¹¶ GPT åˆ†æç»“æœåˆ° eu_location_by_country.json
    ç±»ä¼¼äº eu_user_analysis.py ä¸­çš„ merge_and_report
    """
    # è¯»å–ç°æœ‰çš„æŒ‰å›½å®¶åˆ†ç±»çš„ä½ç½®
    with open(os.path.join(base_dir, "eu_location_by_country.json"), "r") as f:
        country_locations = json.load(f)
    
    # è¯»å– LLM åˆ†æç»“æœ
    llm_result_path = os.path.join( "eu_country_gpt_analysis", "llm_result.parquet")
    if not os.path.exists(llm_result_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ° LLM ç»“æœæ–‡ä»¶: {llm_result_path}")
        return
    
    llm_result = pd.read_parquet(llm_result_path)
    
    print("LLM ç»“æœç»Ÿè®¡:")
    print(llm_result["result"].value_counts())
    
    # åˆå¹¶ç»“æœï¼šå°† LLM è¯†åˆ«çš„å›½å®¶æ·»åŠ åˆ°å¯¹åº”å›½å®¶çš„åˆ—è¡¨ä¸­
    for _, row in llm_result.iterrows():
        location = row["location"]
        iso_code = row["result"]  # ISO ä»£ç æˆ– "unknown"
        
        # è·³è¿‡æ— æ•ˆçš„ ISO ä»£ç 
        if not iso_code or iso_code == "None" or pd.isna(iso_code):
            iso_code = "unknown"
        
        # ç¡®ä¿ ISO ä»£ç æ˜¯å­—ç¬¦ä¸²
        iso_code = str(iso_code).strip()
        
        # æ ‡å‡†åŒ– UK -> GB
        if iso_code == "UK":
            iso_code = "GB"
        
        # å¦‚æœ ISO ä»£ç ä¸åœ¨ç°æœ‰å­—å…¸ä¸­ï¼Œåˆ›å»ºæ–°é”®
        if iso_code not in country_locations:
            country_locations[iso_code] = []
        
        # å¦‚æœä½ç½®ä¸åœ¨è¯¥å›½å®¶çš„åˆ—è¡¨ä¸­ï¼Œæ·»åŠ å®ƒ
        if location not in country_locations[iso_code]:
            country_locations[iso_code].append(location)
        
        # å¦‚æœ LLM è¯†åˆ«å‡ºäº†å›½å®¶ï¼ˆä¸æ˜¯ unknownï¼‰ï¼Œä» unknown ä¸­ç§»é™¤è¯¥ä½ç½®
        if iso_code != "unknown" and "unknown" in country_locations:
            if location in country_locations["unknown"]:
                country_locations["unknown"].remove(location)
    
    # æ¸…ç†ç©ºçš„ unknown åˆ—è¡¨
    if "unknown" in country_locations and len(country_locations["unknown"]) == 0:
        del country_locations["unknown"]
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n=== åˆå¹¶åç»Ÿè®¡ ===")
    for iso, locations in sorted(country_locations.items(), key=lambda x: len(x[1]), reverse=True):
        country_name = iso_to_country_name.get(iso, iso)
        print(f"{iso} ({country_name}): {len(locations)} ä¸ªä½ç½®")
    
    # ä¿å­˜æ›´æ–°åçš„ç»“æœ
    output_file = os.path.join(base_dir, "eu_location_by_country.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(country_locations, f, indent=2, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def get_country_user_ids():
    """
    æŒ‰å›½å®¶è·å–ç”¨æˆ· ID
    è¿”å›å­—å…¸ï¼š{ISOä»£ç : [ç”¨æˆ·IDåˆ—è¡¨]}
    ç±»ä¼¼äº eu_user_analysis.py ä¸­çš„ get_eu_user_idsï¼Œä½†æŒ‰å›½å®¶åˆ†ç»„
    """
    # è¯»å–æŒ‰å›½å®¶åˆ†ç±»çš„ä½ç½®
    with open(os.path.join(base_dir, "eu_location_by_country.json"), "r") as f:
        country_locations = json.load(f)
    
    # è·å–æ‰€æœ‰ feature æ–‡ä»¶
    all_feature_files = glob.glob(os.path.join(feature_dir, "user-*.parquet"))
    
    # æŒ‰å›½å®¶å­˜å‚¨ç”¨æˆ· ID
    country_user_ids = {}
    
    print(f"å¼€å§‹å¤„ç† {len(all_feature_files)} ä¸ª feature æ–‡ä»¶...")
    
    for feature_file in all_feature_files:
        df = pd.read_parquet(feature_file)
        df = df[df["location"].notna()]
        
        # éå†æ¯ä¸ªå›½å®¶
        for iso_code, locations in country_locations.items():
            if iso_code not in country_user_ids:
                country_user_ids[iso_code] = set()
            
            # æ‰¾åˆ°è¯¥å›½å®¶çš„ä½ç½®å¯¹åº”çš„ç”¨æˆ·
            country_df = df[df["location"].isin(locations)]
            country_user_ids[iso_code].update(country_df["id"].unique())
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ç»Ÿè®¡
    print("\n=== æŒ‰å›½å®¶ç”¨æˆ·ç»Ÿè®¡ ===")
    country_user_ids_list = {}
    for iso_code, user_ids in country_user_ids.items():
        user_ids_list = list(user_ids)
        country_user_ids_list[iso_code] = user_ids_list
        country_name = iso_to_country_name.get(iso_code, iso_code)
        print(f"{iso_code} ({country_name}): {len(user_ids_list)} ä¸ªç”¨æˆ·")
    
    # ä¿å­˜ç»“æœ
    output_file = os.path.join(base_dir, "eu_country_user_ids.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(country_user_ids_list, f, indent=2, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # return country_user_ids_list


def count_users_by_country_in_dataset(data_dir="/scratch/network/yh6580/opinion-correlation/twitter/eu"):
    """
    ç»Ÿè®¡æ•°æ®é›†ä¸­æ¯ä¸ªå›½å®¶çš„ç”¨æˆ·æ•°é‡
    
    Args:
        data_dir: åŒ…å« merged-*.parquet æ–‡ä»¶çš„ç›®å½•
    
    Returns:
        ç”Ÿæˆ CSV æ–‡ä»¶ï¼ŒåŒ…å«æ¯ä¸ªå›½å®¶çš„ç”¨æˆ·æ•°é‡ç»Ÿè®¡
    """
    # è¯»å–æŒ‰å›½å®¶åˆ†ç±»çš„ç”¨æˆ· ID
    country_user_ids_file = os.path.join(base_dir, "eu_country_user_ids.json")
    if not os.path.exists(country_user_ids_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {country_user_ids_file}")
        print("è¯·å…ˆè¿è¡Œ 'python eu_country_user_analysis.py user' ç”Ÿæˆè¯¥æ–‡ä»¶")
        return
    
    print(f"[INFO] åŠ è½½å›½å®¶ç”¨æˆ· ID æ˜ å°„: {country_user_ids_file}")
    with open(country_user_ids_file, "r") as f:
        country_user_ids = json.load(f)
    
    # å°†æ‰€æœ‰å›½å®¶çš„ç”¨æˆ· ID è½¬ä¸º setï¼ˆint ç±»å‹ï¼‰ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    country_user_sets = {}
    for iso_code, user_ids in country_user_ids.items():
        country_user_sets[iso_code] = set(int(uid) for uid in user_ids)
    
    print(f"[INFO] å·²åŠ è½½ {len(country_user_sets)} ä¸ªå›½å®¶çš„ç”¨æˆ· ID")
    
    # ä»æ•°æ®é›†ä¸­æ”¶é›†æ‰€æœ‰å­˜åœ¨çš„ç”¨æˆ· ID
    merged_files = glob.glob(os.path.join(data_dir, "merged-*.parquet"))
    if not merged_files:
        print(f"é”™è¯¯: åœ¨ {data_dir} ä¸­æ‰¾ä¸åˆ° merged-*.parquet æ–‡ä»¶")
        return
    
    print(f"[INFO] æ‰¾åˆ° {len(merged_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    # æ”¶é›†æ•°æ®é›†ä¸­æ‰€æœ‰å­˜åœ¨çš„ç”¨æˆ· IDï¼ˆå»é‡ï¼‰
    dataset_user_ids = set()
    for file_path in merged_files:
        print(f"[INFO] å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        try:
            df = pd.read_parquet(file_path, engine="fastparquet")
            # ç¡®ä¿ index æ˜¯ int ç±»å‹
            df.index = pd.to_numeric(df.index, errors="coerce").astype("Int64")
            # å»é™¤ NaN
            df = df[df.index.notna()]
            df.index = df.index.astype("int64")
            # æ·»åŠ åˆ°é›†åˆä¸­
            dataset_user_ids.update(df.index.unique())
        except Exception as e:
            print(f"[WARNING] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"[INFO] æ•°æ®é›†ä¸­å…±æœ‰ {len(dataset_user_ids)} ä¸ªå”¯ä¸€ç”¨æˆ· ID")
    
    # ç»Ÿè®¡æ¯ä¸ªå›½å®¶åœ¨æ•°æ®é›†ä¸­å®é™…å­˜åœ¨çš„ç”¨æˆ·æ•°é‡
    country_counts = []
    for iso_code, user_ids_set in country_user_sets.items():
        # è®¡ç®—äº¤é›†ï¼šè¯¥å›½å®¶çš„ç”¨æˆ· ID ä¸æ•°æ®é›†ä¸­çš„ç”¨æˆ· ID çš„äº¤é›†
        users_in_dataset = user_ids_set & dataset_user_ids
        count = len(users_in_dataset)
        
        country_name = iso_to_country_name.get(iso_code, iso_code)
        country_counts.append({
            "country_iso": iso_code,
            "country_name": country_name,
            "user_count": count
        })
        
        print(f"{iso_code} ({country_name}): {count} ä¸ªç”¨æˆ·")
    
    # æŒ‰ç”¨æˆ·æ•°é‡é™åºæ’åº
    country_counts.sort(key=lambda x: x["user_count"], reverse=True)
    
    # åˆ›å»º DataFrame å¹¶ä¿å­˜ä¸º CSV
    df_result = pd.DataFrame(country_counts)
    
    # åªä¿ç•™ ISO å’Œç”¨æˆ·æ•°é‡ä¸¤åˆ—ï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ï¼‰
    df_output = df_result[["country_iso", "country_name", "user_count"]]
    
    output_file = os.path.join(base_dir, "eu_country_user_count_in_dataset.csv")
    df_output.to_csv(output_file, index=False)
    
    print(f"\n[INFO] ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"[INFO] æ€»è®¡: {sum(c['user_count'] for c in country_counts)} ä¸ªç”¨æˆ·åˆ†å¸ƒåœ¨ {len(country_counts)} ä¸ªå›½å®¶")
    
    return df_output


# ---------------------------------------------------------
if __name__ == "__main__":
    fire.Fire({
        "analyze": analyze_eu_locations_by_country,
        "merge": merge_country_results,
        "user": get_country_user_ids,
        "count": count_users_by_country_in_dataset,
    })

